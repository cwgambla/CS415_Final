{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c31d1d13-c2aa-46b8-ad80-2edfa5bb614a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.datasets import ImageFolder\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d31e9782-fe5d-4a18-9211-2c984252eee3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25abb324-5b05-496e-9df3-d4e3c9811b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"archive\"  \n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "dataset = ImageFolder(root=data_dir , transform=transform)\n",
    "# loader = DataLoader(dataset=dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Define split ratios\n",
    "train_ratio = 0.8\n",
    "val_ratio = 0.1\n",
    "test_ratio = 0.1\n",
    "total_size = len(dataset)\n",
    "train_size = int(train_ratio * total_size)\n",
    "test_size = int(test_ratio * total_size)\n",
    "val_size = total_size - test_size - train_size\n",
    "# Perform the split\n",
    "train_dataset, val_dataset,test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Create DataLoader instances\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "231919bb-bf5f-43d8-ba6f-6d6b807329dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cadmin/anaconda3/envs/Data_Science/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/cadmin/anaconda3/envs/Data_Science/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "class ChessPieceRes_18(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ChessPieceRes_18, self).__init__()\n",
    "        self.model = models.resnet18(pretrained=True).to(device)  # Using ResNet18 as a base\n",
    "        in_features = self.model.fc.in_features\n",
    "        self.model.fc = nn.Linear(in_features, num_classes).to(device)  # Adjusting the final layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "num_classes = len(dataset.classes)\n",
    "model = ChessPieceRes_18(num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88647d4d-f904-4e0d-9d79-f10f1426d2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e17d5a7f-14a9-4153-b354-b93614406cb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "271edbff-50da-4024-b9e4-e0a1f03f5e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/17], Loss: 28.8432\n",
      "Validation Accuracy: 59.09%\n",
      "Epoch [2/17], Loss: 12.0061\n",
      "Validation Accuracy: 87.88%\n",
      "Epoch [3/17], Loss: 5.3076\n",
      "Validation Accuracy: 86.36%\n",
      "Epoch [4/17], Loss: 4.4231\n",
      "Validation Accuracy: 86.36%\n",
      "Epoch [5/17], Loss: 5.1227\n",
      "Validation Accuracy: 77.27%\n",
      "Epoch [6/17], Loss: 6.4755\n",
      "Validation Accuracy: 83.33%\n",
      "Epoch [7/17], Loss: 2.6639\n",
      "Validation Accuracy: 86.36%\n",
      "Epoch [8/17], Loss: 3.6617\n",
      "Validation Accuracy: 86.36%\n",
      "Epoch [9/17], Loss: 3.0397\n",
      "Validation Accuracy: 81.82%\n",
      "Epoch [10/17], Loss: 4.0494\n",
      "Validation Accuracy: 83.33%\n",
      "Epoch [11/17], Loss: 1.8031\n",
      "Validation Accuracy: 89.39%\n",
      "Epoch [12/17], Loss: 2.6412\n",
      "Validation Accuracy: 90.91%\n",
      "Epoch [13/17], Loss: 2.6048\n",
      "Validation Accuracy: 83.33%\n",
      "Epoch [14/17], Loss: 2.5544\n",
      "Validation Accuracy: 89.39%\n",
      "Epoch [15/17], Loss: 1.6092\n",
      "Validation Accuracy: 84.85%\n",
      "Epoch [16/17], Loss: 4.3959\n",
      "Validation Accuracy: 86.36%\n",
      "Epoch [17/17], Loss: 1.1821\n",
      "Validation Accuracy: 93.94%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 17\n",
    "loss_res18 = []\n",
    "loss_val_res18 = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images.to(device))\n",
    "        loss = criterion(outputs, labels.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    loss_res18.append(epoch_loss)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}')\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    running_loss_val = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            outputs = model(images.to(device))\n",
    "            loss_val = criterion(outputs, labels.to(device))\n",
    "            running_loss_val += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels.to(device)).sum().item()\n",
    "\n",
    "        epoch_loss_val = running_loss_val / len(val_loader)\n",
    "        loss_val_res18.append(epoch_loss_val)\n",
    "    val_accuracy = 100 * correct / total\n",
    "    print(f'Validation Accuracy: {val_accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7faf1747-f967-4df8-a670-a9df80d59a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 95.38%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images.to(device))\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels.to(device)).sum().item()\n",
    "\n",
    "test_accuracy = 100 * correct / total\n",
    "print(f'Test Accuracy: {test_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01e75e55-a291-4d4b-8462-6225b5d022ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cadmin/anaconda3/envs/Data_Science/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "class ChessPieceRes_50(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ChessPieceRes_50, self).__init__()\n",
    "        self.model = models.resnet50(pretrained=True).to(device)  # Using ResNet18 as a base\n",
    "        in_features = self.model.fc.in_features\n",
    "        self.model.fc = nn.Linear(in_features, num_classes).to(device)  # Adjusting the final layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "num_classes = len(dataset.classes)\n",
    "model = ChessPieceRes_50(num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ce5256d-d202-481f-bcfe-c22ef8a2d77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c3addd-3f5e-4813-b008-689e524b257d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/17], Loss: 31.0472\n",
      "Validation Accuracy: 54.55%\n",
      "Epoch [2/17], Loss: 18.5953\n",
      "Validation Accuracy: 57.58%\n",
      "Epoch [3/17], Loss: 11.4994\n",
      "Validation Accuracy: 66.67%\n",
      "Epoch [4/17], Loss: 11.3418\n",
      "Validation Accuracy: 50.00%\n",
      "Epoch [5/17], Loss: 7.2003\n",
      "Validation Accuracy: 66.67%\n",
      "Epoch [6/17], Loss: 15.9957\n",
      "Validation Accuracy: 81.82%\n",
      "Epoch [7/17], Loss: 8.8946\n",
      "Validation Accuracy: 71.21%\n",
      "Epoch [8/17], Loss: 8.8727\n",
      "Validation Accuracy: 84.85%\n",
      "Epoch [9/17], Loss: 6.3962\n",
      "Validation Accuracy: 80.30%\n",
      "Epoch [10/17], Loss: 7.0612\n",
      "Validation Accuracy: 78.79%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 17\n",
    "loss_res50 = []\n",
    "loss_val_res50 = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images.to(device))\n",
    "        loss = criterion(outputs, labels.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    loss_res50.append(epoch_loss)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}')\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    running_loss_val = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            outputs = model(images.to(device))\n",
    "            loss_val = criterion(outputs, labels.to(device))\n",
    "            running_loss_val += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels.to(device)).sum().item()\n",
    "\n",
    "        epoch_loss_val = running_loss_val / len(val_loader)\n",
    "        loss_val_res50.append(epoch_loss_val)\n",
    "    val_accuracy = 100 * correct / total\n",
    "    print(f'Validation Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361f16d6-0531-48d2-b074-88e6fbdaa92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images.to(device))\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels.to(device)).sum().item()\n",
    "\n",
    "test_accuracy = 100 * correct / total\n",
    "print(f'Test Accuracy: {test_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e7ed0f-efdb-46f3-a37a-598cb98941e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c48a97-b7e3-46fe-b1c0-6c2803ba2a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self, num_classes, device):\n",
    "        super(CustomCNN, self).__init__()\n",
    "\n",
    "        # First Convolutional Block\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1).to(device) \n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1).to(device) \n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1).to(device) \n",
    "        # self.conv4 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1).to(device) \n",
    "        # self.conv5 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1).to(device) \n",
    "        \n",
    "        # Max Pooling Layer\n",
    "        self.pool = nn.MaxPool2d(2, 2).to(device) \n",
    "\n",
    "        # Fully Connected Layer to map features to num_classes\n",
    "        # Calculate the size after all convolutional and pooling layers\n",
    "        self.fc1 = nn.Linear(512 * 8 * 8, 512).to(device)  # Updated for 512 * 8 * 8 after 5 conv layers\n",
    "        self.fc2 = nn.Linear(512, num_classes).to(device)\n",
    "\n",
    "        # Dropout layer for regularization\n",
    "        self.dropout = nn.Dropout(0.5).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass through convolutional blocks\n",
    "        x = F.relu(self.conv1(x))  # Conv1\n",
    "        x = F.relu(self.conv2(x))  # Conv2\n",
    "        x = F.relu(self.conv3(x))  # Conv3\n",
    "        # x = F.relu(self.conv4(x))  # Conv4\n",
    "        # x = F.relu(self.conv5(x))  # Conv5\n",
    "\n",
    "        # Apply pooling after all conv layers\n",
    "        x = self.pool(x)  # Conv5 + Pool\n",
    "\n",
    "        # Flatten the output from conv layers: (batch_size, 512, 8, 8)\n",
    "        x = x.view(-1, 512 * 8 * 8)  # Adjusted dimension after 5 layers of conv and pooling\n",
    "\n",
    "        # Pass through fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "# Instantiate the model\n",
    "num_classes = len(dataset.classes)  # Replace with your number of classes\n",
    "model = CustomCNN(num_classes=num_classes,device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3a8d43-69f7-45df-88bd-90fd5d90a880",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db5266c-87a6-441e-a295-5ca2d6e88721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_epochs = 25\n",
    "# loss_custom = []\n",
    "# loss_val_custom = []\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     running_loss = 0.0\n",
    "\n",
    "#     for images, labels in train_loader:\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(images.to(device))\n",
    "#         loss = criterion(outputs, labels.to(device))\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         running_loss += loss.item() * images.size(0)\n",
    "\n",
    "#     epoch_loss = running_loss / len(train_loader)\n",
    "#     loss_custom.append(epoch_loss)\n",
    "#     print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}')\n",
    "\n",
    "#     # Validation\n",
    "#     model.eval()\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "    \n",
    "#     running_loss_val = 0.0\n",
    "#     with torch.no_grad():\n",
    "#         for images, labels in val_loader:\n",
    "#             outputs = model(images.to(device))\n",
    "#             loss_val = criterion(outputs, labels.to(device))\n",
    "#             running_loss_val += loss.item() * images.size(0)\n",
    "#             _, predicted = torch.max(outputs.data, 1)\n",
    "#             total += labels.size(0)\n",
    "#             correct += (predicted == labels.to(device)).sum().item()\n",
    "\n",
    "#         epoch_loss_val = running_loss_val / len(val_loader)\n",
    "#         loss_val_custom.append(epoch_loss_val)\n",
    "#     val_accuracy = 100 * correct / total\n",
    "#     print(f'Validation Accuracy: {val_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b8231b-bc55-4b6f-b79b-1e7486d75155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# correct = 0\n",
    "# total = 0\n",
    "# with torch.no_grad():\n",
    "#     for images, labels in test_loader:\n",
    "#         outputs = model(images.to(device))\n",
    "#         _, predicted = torch.max(outputs.data, 1)\n",
    "#         total += labels.size(0)\n",
    "#         correct += (predicted == labels.to(device)).sum().item()\n",
    "\n",
    "# test_accuracy = 100 * correct / total\n",
    "# print(f'Test Accuracy: {test_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab18d8a-e781-4638-aec4-02db737341c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_res18,color='b', label=\"res_18 loss\")\n",
    "plt.plot(loss_res50,color='r', label=\"res_50 loss\")\n",
    "plt.plot(loss_val_res18,color='g',label=\"res18 val loss\")\n",
    "plt.plot(loss_val_res50,color=(0,1,0.5),label=\"res50 val loss\")\n",
    "plt.title(\"Cost\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss CE\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff80aba5-99b5-492d-9fbf-ac2da8d4df3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
