{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c31d1d13-c2aa-46b8-ad80-2edfa5bb614a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.datasets import ImageFolder\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d31e9782-fe5d-4a18-9211-2c984252eee3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25abb324-5b05-496e-9df3-d4e3c9811b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"archive\"  \n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "dataset = ImageFolder(root=data_dir , transform=transform)\n",
    "# loader = DataLoader(dataset=dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Define split ratios\n",
    "train_ratio = 0.8\n",
    "val_ratio = 0.1\n",
    "test_ratio = 0.1\n",
    "total_size = len(dataset)\n",
    "train_size = int(train_ratio * total_size)\n",
    "test_size = int(test_ratio * total_size)\n",
    "val_size = total_size - test_size - train_size\n",
    "# Perform the split\n",
    "train_dataset, val_dataset,test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Create DataLoader instances\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "231919bb-bf5f-43d8-ba6f-6d6b807329dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cadmin/anaconda3/envs/Data_Science/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/cadmin/anaconda3/envs/Data_Science/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "class ChessPieceRes_18(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ChessPieceRes_18, self).__init__()\n",
    "        self.model = models.resnet18(pretrained=True).to(device)  # Using ResNet18 as a base\n",
    "        in_features = self.model.fc.in_features\n",
    "        self.model.fc = nn.Linear(in_features, num_classes).to(device)  # Adjusting the final layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "num_classes = len(dataset.classes)\n",
    "model = ChessPieceRes_18(num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88647d4d-f904-4e0d-9d79-f10f1426d2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e17d5a7f-14a9-4153-b354-b93614406cb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "271edbff-50da-4024-b9e4-e0a1f03f5e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/25], Loss: 26.3080\n",
      "Validation Accuracy: 72.73%\n",
      "Epoch [2/25], Loss: 12.0818\n",
      "Validation Accuracy: 87.88%\n",
      "Epoch [3/25], Loss: 6.3183\n",
      "Validation Accuracy: 90.91%\n",
      "Epoch [4/25], Loss: 3.2691\n",
      "Validation Accuracy: 86.36%\n",
      "Epoch [5/25], Loss: 2.5382\n",
      "Validation Accuracy: 93.94%\n",
      "Epoch [6/25], Loss: 5.8869\n",
      "Validation Accuracy: 90.91%\n",
      "Epoch [7/25], Loss: 4.9273\n",
      "Validation Accuracy: 77.27%\n",
      "Epoch [8/25], Loss: 2.4877\n",
      "Validation Accuracy: 90.91%\n",
      "Epoch [9/25], Loss: 2.1870\n",
      "Validation Accuracy: 89.39%\n",
      "Epoch [10/25], Loss: 1.4995\n",
      "Validation Accuracy: 78.79%\n",
      "Epoch [11/25], Loss: 1.1501\n",
      "Validation Accuracy: 92.42%\n",
      "Epoch [12/25], Loss: 0.7886\n",
      "Validation Accuracy: 89.39%\n",
      "Epoch [13/25], Loss: 0.9077\n",
      "Validation Accuracy: 92.42%\n",
      "Epoch [14/25], Loss: 2.5788\n",
      "Validation Accuracy: 80.30%\n",
      "Epoch [15/25], Loss: 5.2221\n",
      "Validation Accuracy: 74.24%\n",
      "Epoch [16/25], Loss: 3.6646\n",
      "Validation Accuracy: 86.36%\n",
      "Epoch [17/25], Loss: 4.4900\n",
      "Validation Accuracy: 87.88%\n",
      "Epoch [18/25], Loss: 2.0248\n",
      "Validation Accuracy: 90.91%\n",
      "Epoch [19/25], Loss: 1.1064\n",
      "Validation Accuracy: 89.39%\n",
      "Epoch [20/25], Loss: 0.8821\n",
      "Validation Accuracy: 84.85%\n",
      "Epoch [21/25], Loss: 4.8438\n",
      "Validation Accuracy: 66.67%\n",
      "Epoch [22/25], Loss: 7.6923\n",
      "Validation Accuracy: 71.21%\n",
      "Epoch [23/25], Loss: 8.4210\n",
      "Validation Accuracy: 54.55%\n",
      "Epoch [24/25], Loss: 7.8878\n",
      "Validation Accuracy: 84.85%\n",
      "Epoch [25/25], Loss: 2.9644\n",
      "Validation Accuracy: 86.36%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 25\n",
    "loss_res18 = []\n",
    "loss_val_res18 = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images.to(device))\n",
    "        loss = criterion(outputs, labels.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    loss_res18.append(epoch_loss)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}')\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    running_loss_val = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            outputs = model(images.to(device))\n",
    "            loss_val = criterion(outputs, labels.to(device))\n",
    "            running_loss_val += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels.to(device)).sum().item()\n",
    "\n",
    "        epoch_loss_val = running_loss_val / len(val_loader)\n",
    "        loss_val_res18.append(epoch_loss_val)\n",
    "    val_accuracy = 100 * correct / total\n",
    "    print(f'Validation Accuracy: {val_accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7faf1747-f967-4df8-a670-a9df80d59a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 86.15%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images.to(device))\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels.to(device)).sum().item()\n",
    "\n",
    "test_accuracy = 100 * correct / total\n",
    "print(f'Test Accuracy: {test_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01e75e55-a291-4d4b-8462-6225b5d022ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cadmin/anaconda3/envs/Data_Science/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "class ChessPieceRes_50(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ChessPieceRes_50, self).__init__()\n",
    "        self.model = models.resnet50(pretrained=True).to(device)  # Using ResNet18 as a base\n",
    "        in_features = self.model.fc.in_features\n",
    "        self.model.fc = nn.Linear(in_features, num_classes).to(device)  # Adjusting the final layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "num_classes = len(dataset.classes)\n",
    "model = ChessPieceRes_50(num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ce5256d-d202-481f-bcfe-c22ef8a2d77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05c3addd-3f5e-4813-b008-689e524b257d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/25], Loss: 32.8264\n",
      "Validation Accuracy: 40.91%\n",
      "Epoch [2/25], Loss: 21.6636\n",
      "Validation Accuracy: 33.33%\n",
      "Epoch [3/25], Loss: 16.9225\n",
      "Validation Accuracy: 66.67%\n",
      "Epoch [4/25], Loss: 12.8855\n",
      "Validation Accuracy: 68.18%\n",
      "Epoch [5/25], Loss: 9.6514\n",
      "Validation Accuracy: 74.24%\n",
      "Epoch [6/25], Loss: 9.4778\n",
      "Validation Accuracy: 74.24%\n",
      "Epoch [7/25], Loss: 14.3898\n",
      "Validation Accuracy: 74.24%\n",
      "Epoch [8/25], Loss: 6.9120\n",
      "Validation Accuracy: 80.30%\n",
      "Epoch [9/25], Loss: 7.7063\n",
      "Validation Accuracy: 77.27%\n",
      "Epoch [10/25], Loss: 5.5728\n",
      "Validation Accuracy: 83.33%\n",
      "Epoch [11/25], Loss: 7.0825\n",
      "Validation Accuracy: 42.42%\n",
      "Epoch [12/25], Loss: 7.0077\n",
      "Validation Accuracy: 75.76%\n",
      "Epoch [13/25], Loss: 2.8671\n",
      "Validation Accuracy: 86.36%\n",
      "Epoch [14/25], Loss: 2.9643\n",
      "Validation Accuracy: 80.30%\n",
      "Epoch [15/25], Loss: 3.1707\n",
      "Validation Accuracy: 78.79%\n",
      "Epoch [16/25], Loss: 3.5752\n",
      "Validation Accuracy: 80.30%\n",
      "Epoch [17/25], Loss: 3.7491\n",
      "Validation Accuracy: 77.27%\n",
      "Epoch [18/25], Loss: 7.9501\n",
      "Validation Accuracy: 75.76%\n",
      "Epoch [19/25], Loss: 6.1314\n",
      "Validation Accuracy: 81.82%\n",
      "Epoch [20/25], Loss: 3.0846\n",
      "Validation Accuracy: 80.30%\n",
      "Epoch [21/25], Loss: 5.3583\n",
      "Validation Accuracy: 75.76%\n",
      "Epoch [22/25], Loss: 5.1000\n",
      "Validation Accuracy: 86.36%\n",
      "Epoch [23/25], Loss: 7.5060\n",
      "Validation Accuracy: 77.27%\n",
      "Epoch [24/25], Loss: 4.9169\n",
      "Validation Accuracy: 48.48%\n",
      "Epoch [25/25], Loss: 9.0637\n",
      "Validation Accuracy: 68.18%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 25\n",
    "loss_res50 = []\n",
    "loss_val_res50 = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images.to(device))\n",
    "        loss = criterion(outputs, labels.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    loss_res50.append(epoch_loss)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}')\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    running_loss_val = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            outputs = model(images.to(device))\n",
    "            loss_val = criterion(outputs, labels.to(device))\n",
    "            running_loss_val += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels.to(device)).sum().item()\n",
    "\n",
    "        epoch_loss_val = running_loss_val / len(val_loader)\n",
    "        loss_val_res50.append(epoch_loss_val)\n",
    "    val_accuracy = 100 * correct / total\n",
    "    print(f'Validation Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "361f16d6-0531-48d2-b074-88e6fbdaa92d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 75.38%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images.to(device))\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels.to(device)).sum().item()\n",
    "\n",
    "test_accuracy = 100 * correct / total\n",
    "print(f'Test Accuracy: {test_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d8e7ed0f-efdb-46f3-a37a-598cb98941e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "02c48a97-b7e3-46fe-b1c0-6c2803ba2a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self, num_classes, device):\n",
    "        super(CustomCNN, self).__init__()\n",
    "\n",
    "        # First Convolutional Block\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1).to(device) \n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1).to(device) \n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1).to(device) \n",
    "        # self.conv4 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1).to(device) \n",
    "        # self.conv5 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1).to(device) \n",
    "        \n",
    "        # Max Pooling Layer\n",
    "        self.pool = nn.MaxPool2d(2, 2).to(device) \n",
    "\n",
    "        # Fully Connected Layer to map features to num_classes\n",
    "        # Calculate the size after all convolutional and pooling layers\n",
    "        self.fc1 = nn.Linear(512 * 8 * 8, 512).to(device)  # Updated for 512 * 8 * 8 after 5 conv layers\n",
    "        self.fc2 = nn.Linear(512, num_classes).to(device)\n",
    "\n",
    "        # Dropout layer for regularization\n",
    "        self.dropout = nn.Dropout(0.5).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass through convolutional blocks\n",
    "        x = F.relu(self.conv1(x))  # Conv1\n",
    "        x = F.relu(self.conv2(x))  # Conv2\n",
    "        x = F.relu(self.conv3(x))  # Conv3\n",
    "        # x = F.relu(self.conv4(x))  # Conv4\n",
    "        # x = F.relu(self.conv5(x))  # Conv5\n",
    "\n",
    "        # Apply pooling after all conv layers\n",
    "        x = self.pool(x)  # Conv5 + Pool\n",
    "\n",
    "        # Flatten the output from conv layers: (batch_size, 512, 8, 8)\n",
    "        x = x.view(-1, 512 * 8 * 8)  # Adjusted dimension after 5 layers of conv and pooling\n",
    "\n",
    "        # Pass through fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "# Instantiate the model\n",
    "num_classes = len(dataset.classes)  # Replace with your number of classes\n",
    "model = CustomCNN(num_classes=num_classes,device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d3a8d43-69f7-45df-88bd-90fd5d90a880",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7db5266c-87a6-441e-a295-5ca2d6e88721",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (2048) to match target batch_size (32).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     11\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(images\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[0;32m---> 12\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[1;32m     13\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/anaconda3/envs/Data_Science/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/Data_Science/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/Data_Science/lib/python3.11/site-packages/torch/nn/modules/loss.py:1188\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mcross_entropy(\u001b[38;5;28minput\u001b[39m, target, weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight,\n\u001b[1;32m   1189\u001b[0m                            ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_index, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction,\n\u001b[1;32m   1190\u001b[0m                            label_smoothing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_smoothing)\n",
      "File \u001b[0;32m~/anaconda3/envs/Data_Science/lib/python3.11/site-packages/torch/nn/functional.py:3104\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3103\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mcross_entropy_loss(\u001b[38;5;28minput\u001b[39m, target, weight, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (2048) to match target batch_size (32)."
     ]
    }
   ],
   "source": [
    "# num_epochs = 25\n",
    "# loss_custom = []\n",
    "# loss_val_custom = []\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     running_loss = 0.0\n",
    "\n",
    "#     for images, labels in train_loader:\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(images.to(device))\n",
    "#         loss = criterion(outputs, labels.to(device))\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         running_loss += loss.item() * images.size(0)\n",
    "\n",
    "#     epoch_loss = running_loss / len(train_loader)\n",
    "#     loss_custom.append(epoch_loss)\n",
    "#     print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}')\n",
    "\n",
    "#     # Validation\n",
    "#     model.eval()\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "    \n",
    "#     running_loss_val = 0.0\n",
    "#     with torch.no_grad():\n",
    "#         for images, labels in val_loader:\n",
    "#             outputs = model(images.to(device))\n",
    "#             loss_val = criterion(outputs, labels.to(device))\n",
    "#             running_loss_val += loss.item() * images.size(0)\n",
    "#             _, predicted = torch.max(outputs.data, 1)\n",
    "#             total += labels.size(0)\n",
    "#             correct += (predicted == labels.to(device)).sum().item()\n",
    "\n",
    "#         epoch_loss_val = running_loss_val / len(val_loader)\n",
    "#         loss_val_custom.append(epoch_loss_val)\n",
    "#     val_accuracy = 100 * correct / total\n",
    "#     print(f'Validation Accuracy: {val_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c3b8231b-bc55-4b6f-b79b-1e7486d75155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 30.77%\n"
     ]
    }
   ],
   "source": [
    "# model.eval()\n",
    "# correct = 0\n",
    "# total = 0\n",
    "# with torch.no_grad():\n",
    "#     for images, labels in test_loader:\n",
    "#         outputs = model(images.to(device))\n",
    "#         _, predicted = torch.max(outputs.data, 1)\n",
    "#         total += labels.size(0)\n",
    "#         correct += (predicted == labels.to(device)).sum().item()\n",
    "\n",
    "# test_accuracy = 100 * correct / total\n",
    "# print(f'Test Accuracy: {test_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4ab18d8a-e781-4638-aec4-02db737341c8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loss_res18' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(loss_res18,color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mres_18 loss\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(loss_res50,color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mres_50 loss\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(loss_val_res18,color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mg\u001b[39m\u001b[38;5;124m'\u001b[39m,label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mres18 val loss\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loss_res18' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(loss_res18,color='b', label=\"res_18 loss\")\n",
    "plt.plot(loss_res50,color='r', label=\"res_50 loss\")\n",
    "plt.plot(loss_val_res18,color='g',label=\"res18 val loss\")\n",
    "plt.plot(loss_val_res50,color=(0,1,0.5),label=\"res50 val loss\")\n",
    "plt.title(\"Cost\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss CE\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff80aba5-99b5-492d-9fbf-ac2da8d4df3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
